# RAG Workflow Documentation

> **CV Screener AI - Complete RAG Pipeline Reference**
> 
> Version: 4.0.0 | Last Updated: January 2026

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Diagram](#architecture-diagram)
3. [Pipeline Stages](#pipeline-stages)
4. [Core Scripts Reference](#core-scripts-reference)
5. [Data Flow](#data-flow)
6. [Configuration](#configuration)
7. [Providers](#providers)
8. [Error Handling](#error-handling)
9. [Caching & Performance](#caching--performance)
10. [Evaluation & Logging](#evaluation--logging)

---

## System Overview

The CV Screener uses a **multi-step RAG (Retrieval-Augmented Generation) pipeline** designed for intelligent CV analysis and candidate screening. The system supports two operation modes:

| Mode | Description |
|------|-------------|
| **LOCAL** | In-memory vector store, local embeddings |
| **CLOUD** | Supabase pgvector, OpenAI embeddings, OpenRouter LLMs |

### Key Features

- âœ… **2-Step LLM Architecture**: Fast model for query understanding + powerful model for generation
- âœ… **Guardrails**: Pre-LLM filtering to reject off-topic queries
- âœ… **Hallucination Detection**: Post-LLM verification against context
- âœ… **Adaptive Retrieval**: Strategy varies based on query type and session size
- âœ… **LLM-based Reranking**: Re-orders chunks by semantic relevance
- âœ… **Circuit Breaker**: Prevents cascading failures
- âœ… **Response Caching**: LRU cache with TTL for embeddings and responses
- âœ… **Evaluation Logging**: JSONL logs for continuous improvement

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              USER QUERY                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: QUERY UNDERSTANDING                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Model: google/gemini-2.0-flash-001 (fast, cheap)                   â”‚  â”‚
â”‚  â”‚ â€¢ Extracts: query_type, requirements, is_cv_related                  â”‚  â”‚
â”‚  â”‚ â€¢ Reformulates query for better retrieval                            â”‚  â”‚
â”‚  â”‚ â€¢ Output: QueryUnderstanding dataclass                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  Script: query_understanding_service.py                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: GUARDRAIL CHECK                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Keyword matching: CV_KEYWORDS set (100+ terms)                     â”‚  â”‚
â”‚  â”‚ â€¢ Pattern matching: OFF_TOPIC_PATTERNS (recipes, weather, etc.)      â”‚  â”‚
â”‚  â”‚ â€¢ Fast, no LLM call required                                         â”‚  â”‚
â”‚  â”‚ â€¢ Output: GuardrailResult (is_allowed, rejection_message)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  Script: guardrail_service.py                                              â”‚
â”‚                                                                            â”‚
â”‚  âŒ REJECTED â†’ Return early with rejection message                         â”‚
â”‚  âœ… PASSED â†’ Continue to next step                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: EMBEDDING GENERATION                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Model: text-embedding-3-small (1536 dimensions)                    â”‚  â”‚
â”‚  â”‚ â€¢ Cache: LRU with TTL (5 min default)                                â”‚  â”‚
â”‚  â”‚ â€¢ Retry: 3 attempts with exponential backoff                         â”‚  â”‚
â”‚  â”‚ â€¢ Timeout: 10 seconds                                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  Script: embedding_service.py                                              â”‚
â”‚  Provider: OpenAI / LocalEmbeddingProvider                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: VECTOR SEARCH (Retrieval)                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ADAPTIVE STRATEGY based on query_type and session size:                 â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â”‚ â”‚ Query Type      â”‚ Strategy           â”‚ K Value                      â”‚ â”‚â”‚
â”‚  â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚â”‚
â”‚  â”‚ â”‚ ranking         â”‚ diversify_by_cv    â”‚ min(num_cvs, 30-100)         â”‚ â”‚â”‚
â”‚  â”‚ â”‚ comparison      â”‚ diversify_by_cv    â”‚ min(num_cvs, 30-100)         â”‚ â”‚â”‚
â”‚  â”‚ â”‚ search (small)  â”‚ diversify_by_cv    â”‚ num_cvs                      â”‚ â”‚â”‚
â”‚  â”‚ â”‚ search (large)  â”‚ top-k precision    â”‚ k (10 default)               â”‚ â”‚â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ â€¢ Threshold: 0.3 default (lowered for large sessions)                   â”‚â”‚
â”‚  â”‚ â€¢ Filters by session_id and optional cv_ids                             â”‚â”‚
â”‚  â”‚ â€¢ Timeout: 15 seconds                                                   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Script: vector_store.py (Cloud: Supabase pgvector, Local: in-memory)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: RERANKING                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ Model: google/gemini-2.0-flash-001                                    â”‚â”‚
â”‚  â”‚ â€¢ Scores each chunk 1-10 for relevance to query                         â”‚â”‚
â”‚  â”‚ â€¢ Combined score: LLM_score * 0.7 + similarity * 0.3                    â”‚â”‚
â”‚  â”‚ â€¢ Returns ALL chunks reordered (not truncated)                          â”‚â”‚
â”‚  â”‚ â€¢ Can be disabled via config                                            â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Script: reranking_service.py                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: RESPONSE GENERATION                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ PROMPT CONSTRUCTION (templates.py):                                     â”‚â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â”‚ â”‚ SYSTEM_PROMPT (Expert HR analyst persona)                           â”‚ â”‚â”‚
â”‚  â”‚ â”‚    +                                                                â”‚ â”‚â”‚
â”‚  â”‚ â”‚ QUERY_TEMPLATE / COMPARISON_TEMPLATE / RANKING_TEMPLATE             â”‚ â”‚â”‚
â”‚  â”‚ â”‚    +                                                                â”‚ â”‚â”‚
â”‚  â”‚ â”‚ Formatted context (chunks with CV IDs and metadata)                 â”‚ â”‚â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ â€¢ Models: gemini-1.5-flash, gemini-1.5-pro, gpt-4o, claude-3           â”‚â”‚
â”‚  â”‚ â€¢ Temperature: 0.1 (for accuracy)                                       â”‚â”‚
â”‚  â”‚ â€¢ Max tokens: 2048-4096                                                 â”‚â”‚
â”‚  â”‚ â€¢ Timeout: 120 seconds                                                  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Script: llm.py (OpenRouterLLMProvider)                                      â”‚
â”‚  Templates: templates.py (PromptBuilder class)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 7: VERIFICATION & HALLUCINATION CHECK                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 7a. LLM VERIFICATION (verification_service.py)                          â”‚â”‚
â”‚  â”‚     â€¢ Uses LLM to check if response is grounded in context              â”‚â”‚
â”‚  â”‚     â€¢ Returns: groundedness_score, verified_claims, ungrounded_claims   â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ 7b. HEURISTIC HALLUCINATION CHECK (hallucination_service.py)            â”‚â”‚
â”‚  â”‚     â€¢ Regex-based verification (no LLM call)                            â”‚â”‚
â”‚  â”‚     â€¢ Checks: CV IDs match context, names exist in CVs                  â”‚â”‚
â”‚  â”‚     â€¢ Returns: confidence_score, verified_cv_ids, warnings              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  If not grounded: Adds warning "âš ï¸ Some information could not be verified" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 8: EVALUATION LOGGING                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ Logs to: eval_logs/queries_YYYYMMDD.jsonl                             â”‚â”‚
â”‚  â”‚ â€¢ Fields: query, response, sources, metrics, hallucination_check        â”‚â”‚
â”‚  â”‚ â€¢ Daily stats aggregation                                               â”‚â”‚
â”‚  â”‚ â€¢ Low confidence tracking (threshold: 0.5)                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Script: eval_service.py                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              RAG RESPONSE                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ {                                                                    â”‚  â”‚
â”‚  â”‚   "answer": "Generated response text...",                            â”‚  â”‚
â”‚  â”‚   "sources": [{"cv_id": "cv_xxx", "filename": "John_Doe.pdf"}],      â”‚  â”‚
â”‚  â”‚   "metrics": {"total_ms": 1234, "stages": {...}},                    â”‚  â”‚
â”‚  â”‚   "confidence_score": 0.85,                                          â”‚  â”‚
â”‚  â”‚   "guardrail_passed": true,                                          â”‚  â”‚
â”‚  â”‚   "mode": "cloud",                                                   â”‚  â”‚
â”‚  â”‚   "request_id": "abc123"                                             â”‚  â”‚
â”‚  â”‚ }                                                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Pipeline Stages

### Stage Enum Definition

```python
class PipelineStage(Enum):
    QUERY_UNDERSTANDING = auto()  # Step 1
    GUARDRAIL = auto()            # Step 2
    EMBEDDING = auto()            # Step 3
    SEARCH = auto()               # Step 4
    RERANKING = auto()            # Step 5
    GENERATION = auto()           # Step 6
    VERIFICATION = auto()         # Step 7a
    HALLUCINATION_CHECK = auto()  # Step 7b
```

### Stage Metrics

Each stage tracks:
- `duration_ms`: Execution time
- `success`: Boolean status
- `error`: Error message if failed
- `metadata`: Stage-specific data

---

## Core Scripts Reference

### ðŸ“ Orchestration Layer

| Script | Class | Description |
|--------|-------|-------------|
| `rag_service_v3.py` | `RAGServiceV4` | Main orchestrator. Executes pipeline, manages caching, circuit breakers, retry logic. |
| `rag_service_langchain.py` | `LangChainRAGService` | Alternative orchestrator using LangChain LCEL components. |
| `factory.py` | `ProviderFactory` | Factory pattern for provider instantiation based on mode. |

### ðŸ“ Pipeline Steps (in order)

| # | Script | Class | Input â†’ Output |
|---|--------|-------|----------------|
| 1 | `query_understanding_service.py` | `QueryUnderstandingService` | `str` â†’ `QueryUnderstanding` |
| 2 | `guardrail_service.py` | `GuardrailService` | `str` â†’ `GuardrailResult` |
| 3 | `embedding_service.py` | `EmbeddingService` | `str` â†’ `List[float]` |
| 4 | `vector_store.py` | `SupabaseVectorStore` / `SimpleVectorStore` | `List[float]` â†’ `List[SearchResult]` |
| 5 | `reranking_service.py` | `RerankingService` | `List[SearchResult]` â†’ `RerankResult` |
| 6 | `llm.py` | `OpenRouterLLMProvider` | `prompt: str` â†’ `str` |
| 7a | `verification_service.py` | `LLMVerificationService` | `response + context` â†’ `VerificationResult` |
| 7b | `hallucination_service.py` | `HallucinationService` | `response + context` â†’ `HallucinationCheckResult` |
| 8 | `eval_service.py` | `EvalService` | Logs query/response to JSONL |

### ðŸ“ Support Layer

| Script | Class | Description |
|--------|-------|-------------|
| `templates.py` | `PromptBuilder` | All prompt templates and builder methods |
| `chunking_service.py` | `ChunkingService` | CV text â†’ semantic sections |
| `pdf_service.py` | `PDFService` | PDF â†’ text extraction |
| `base.py` | `EmbeddingProvider`, `VectorStoreProvider`, `LLMProvider` | Abstract interfaces |

---

## Data Flow

### CV Ingestion Flow

```
PDF Upload
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Service   â”‚ â†’ Extract text from PDF
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunking Serviceâ”‚ â†’ Split into sections (experience, education, skills...)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Embedding Serviceâ”‚ â†’ Generate vector for each chunk
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Store   â”‚ â†’ Store in Supabase with metadata
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Flow

```
User Question: "Who has Python experience?"
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QueryUnderstanding:                                            â”‚
â”‚   query_type: "search"                                         â”‚
â”‚   requirements: ["Search for Python skill"]                    â”‚
â”‚   reformulated_prompt: "Find candidates with Python..."        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Guardrail: PASSED (contains CV keywords)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Search:                                                 â”‚
â”‚   Strategy: top-k (search query, large session)                â”‚
â”‚   Results: 10 chunks from 5 different CVs                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reranking:                                                     â”‚
â”‚   Scores: [9.5, 8.2, 7.8, 6.5, ...]                            â”‚
â”‚   Reordered by relevance                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Generation:                                                â”‚
â”‚   SYSTEM_PROMPT + QUERY_TEMPLATE + formatted chunks            â”‚
â”‚   â†’ "Based on the CVs, the following candidates have Python    â”‚
â”‚      experience: [CV:cv_abc123] John Doe (5 years)..."         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Configuration

### RAGConfig Dataclass

```python
@dataclass
class RAGConfig:
    mode: Mode = Mode.LOCAL
    
    # Model configuration
    understanding_model: str | None = None      # Default: gemini-2.0-flash-001
    reranking_model: str | None = None          # Default: gemini-2.0-flash-001
    generation_model: str | None = None         # Default: gemini-1.5-flash
    verification_model: str | None = None       # Default: gemini-2.0-flash-001
    
    # Feature flags
    reranking_enabled: bool = True
    verification_enabled: bool = True
    streaming_enabled: bool = False
    parallel_steps_enabled: bool = True
    
    # Retrieval settings
    default_k: int = 10
    default_threshold: float = 0.3
    max_context_tokens: int = 60000
    
    # Timeouts (seconds)
    embedding_timeout: float = 10.0
    search_timeout: float = 15.0
    llm_timeout: float = 120.0
    total_timeout: float = 180.0
```

### Environment Variables

```bash
# Required
OPENAI_API_KEY=your_openai_key           # For embeddings
OPENROUTER_API_KEY=your_openrouter_key    # For LLM generation

# Optional (Cloud mode)
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_service_key
GOOGLE_API_KEY=...              # For LangChain Gemini

# Feature flags
USE_LANGCHAIN=false             # Use LangChain wrapper
```

---

## Providers

### Provider Interface

```python
class EmbeddingProvider(ABC):
    async def embed_query(self, text: str) -> EmbeddingResult
    async def embed_documents(self, texts: List[str]) -> EmbeddingResult

class VectorStoreProvider(ABC):
    async def search(self, embedding, k, threshold, cv_ids, diversify_by_cv) -> List[SearchResult]
    async def add_embeddings(self, embeddings, metadatas, ids)

class LLMProvider(ABC):
    async def generate(self, prompt: str, system_prompt: str, **kwargs) -> LLMResult
```

### Provider Implementations

| Provider | Mode | Implementation |
|----------|------|----------------|
| `OpenRouterEmbeddingProvider` | Cloud | OpenAI text-embedding-3-small via OpenRouter |
| `LocalEmbeddingProvider` | Local | sentence-transformers (fallback) |
| `SupabaseVectorStore` | Cloud | pgvector in Supabase |
| `SimpleVectorStore` | Local | NumPy cosine similarity |
| `OpenRouterLLMProvider` | Both | OpenRouter API (Gemini, GPT-4, Claude) |

---

## Error Handling

### Error Types

```python
class RAGError(Exception):
    stage: PipelineStage | None
    severity: ErrorSeverity  # WARNING, RECOVERABLE, FATAL
    cause: Exception | None
    recoverable: bool

class GuardrailError(RAGError):      # Query rejected
class RetrievalError(RAGError):      # Search failed
class GenerationError(RAGError):     # LLM failed
```

### Retry Configuration

```python
@dataclass
class RetryConfig:
    max_attempts: int = 3
    base_delay_ms: int = 100
    max_delay_ms: int = 5000
    exponential_base: float = 2.0
    jitter: bool = True
```

### Circuit Breaker

```python
@dataclass
class CircuitBreakerConfig:
    enabled: bool = True
    failure_threshold: int = 5        # Open after 5 failures
    recovery_timeout_seconds: int = 30 # Try recovery after 30s
    half_open_max_calls: int = 3      # Test calls before closing
```

**States:**
- `CLOSED` â†’ Normal operation
- `OPEN` â†’ Failing, rejecting all calls
- `HALF_OPEN` â†’ Testing recovery

---

## Caching & Performance

### LRU Cache

```python
@dataclass
class CacheConfig:
    enabled: bool = True
    ttl_seconds: int = 300      # 5 minutes
    max_entries: int = 1000
    cache_embeddings: bool = True
    cache_responses: bool = True
```

### What Gets Cached

| Item | Cache Key | TTL |
|------|-----------|-----|
| Query embeddings | `emb:{query_text}` | 5 min |
| Full responses | `resp:{query_hash}` | 5 min |

### Performance Metrics

```python
@dataclass
class PipelineMetrics:
    total_ms: float
    stages: list[StageMetrics]
    cache_hit: bool
    retry_count: int
```

---

## Evaluation & Logging

### Query Log Entry

```python
@dataclass
class QueryLogEntry:
    timestamp: str
    session_id: Optional[str]
    query: str
    response: str
    sources: List[Dict[str, Any]]
    metrics: Dict[str, float]
    hallucination_check: Dict[str, Any]
    guardrail_passed: bool
    confidence_score: float
    mode: str
```

### Log Location

```
eval_logs/
â”œâ”€â”€ queries_20260103.jsonl    # Today's queries
â”œâ”€â”€ queries_20260102.jsonl    # Yesterday's queries
â””â”€â”€ ...
```

### Daily Statistics

```python
@dataclass
class DailyStats:
    date: str
    total_queries: int
    avg_confidence: float
    guardrail_rejections: int
    avg_latency_ms: float
    low_confidence_count: int
    unique_sessions: int
```

---

## Prompt Templates

### System Prompt (Persona)

```python
SYSTEM_PROMPT = """You are an expert HR analyst and CV reviewer assistant.
Your job is to analyze CVs and help with candidate screening.

CRITICAL RULES:
1. ONLY use information from the provided CV context
2. NEVER fabricate information not in the CVs
3. Include [CV:cv_id] references for every claim
4. Use Markdown tables when comparing candidates
..."""
```

### Query Templates

| Template | Use Case |
|----------|----------|
| `QUERY_TEMPLATE` | General questions |
| `QUERY_TEMPLATE_CONCISE` | Short answers |
| `QUERY_TEMPLATE_JSON` | Structured JSON output |
| `COMPARISON_TEMPLATE` | Compare multiple candidates |
| `RANKING_TEMPLATE` | Rank candidates by criteria |

### PromptBuilder Class

```python
class PromptBuilder:
    def build_query_prompt(question, chunks, total_cvs, response_format)
    def build_comparison_prompt(criteria, chunks)
    def build_ranking_prompt(role, criteria, chunks, top_n)
```

---

## API Endpoints

### Query Endpoint

```
POST /api/v2/query
{
    "question": "Who has Python experience?",
    "session_id": "session_xxx",
    "k": 10,
    "threshold": 0.3
}

Response:
{
    "answer": "...",
    "sources": [...],
    "metrics": {...},
    "confidence_score": 0.85,
    "guardrail_passed": true
}
```

### Health Check

```
GET /api/health
{
    "status": "ok",
    "mode": "cloud",
    "reranking_enabled": true,
    "verification_enabled": true
}
```

---

## File Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py              # Main API routes
â”‚   â”‚   â”œâ”€â”€ routes_v2.py           # V2 API with sessions
â”‚   â”‚   â”œâ”€â”€ routes_sessions.py     # Session management
â”‚   â”‚   â””â”€â”€ dependencies.py        # FastAPI dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ rag_service_v3.py      # Main RAG orchestrator (RAGServiceV4)
â”‚   â”‚   â”œâ”€â”€ rag_service_langchain.py # LangChain alternative
â”‚   â”‚   â”œâ”€â”€ query_understanding_service.py
â”‚   â”‚   â”œâ”€â”€ guardrail_service.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ reranking_service.py
â”‚   â”‚   â”œâ”€â”€ verification_service.py
â”‚   â”‚   â”œâ”€â”€ hallucination_service.py
â”‚   â”‚   â”œâ”€â”€ chunking_service.py
â”‚   â”‚   â”œâ”€â”€ pdf_service.py
â”‚   â”‚   â””â”€â”€ eval_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ base.py                # Abstract interfaces
â”‚   â”‚   â”œâ”€â”€ factory.py             # Provider factory
â”‚   â”‚   â”œâ”€â”€ cloud/
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”‚   â””â”€â”€ sessions.py
â”‚   â”‚   â””â”€â”€ local/
â”‚   â”‚       â”œâ”€â”€ embeddings.py
â”‚   â”‚       â”œâ”€â”€ llm.py
â”‚   â”‚       â””â”€â”€ vector_store.py
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ templates.py           # All prompt templates
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ schemas.py             # Pydantic models
â”‚   â”‚   â””â”€â”€ sessions.py            # Session management
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py                  # Settings and configuration
â”‚   â””â”€â”€ main.py                    # FastAPI app entry point
â”‚
â”œâ”€â”€ eval_logs/                     # Query logs (JSONL)
â”œâ”€â”€ migrations/                    # SQL migrations
â””â”€â”€ tests/                         # Test suite
```

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 4.0.0 | Jan 2026 | RAGServiceV4 with circuit breaker, LRU cache, pipeline metrics |
| 3.0.0 | Dec 2025 | 2-step LLM, QueryUnderstanding, Reranking |
| 2.0.0 | Nov 2025 | Guardrails, Hallucination detection |
| 1.0.0 | Oct 2025 | Initial RAG pipeline |

---

## Backward Compatibility

```python
# In rag_service_v3.py
RAGServiceV3 = RAGServiceV4  # Alias for backward compatibility
```

Routes and other modules importing `RAGServiceV3` will automatically use the latest `RAGServiceV4` implementation.

---

> **Note**: This document reflects the current state of the RAG system as of January 2026. For updates, refer to the source code and CHANGELOG.
