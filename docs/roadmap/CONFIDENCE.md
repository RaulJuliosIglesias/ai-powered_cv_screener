# Plan de ImplementaciÃ³n: Confidence Scoring Avanzado

## ğŸ”´ ANÃLISIS HONESTO: Lo que tenemos vs. Lo que necesitamos

### Estado Actual (REALIDAD)

| TÃ©cnica | Â¿La tenemos? | CÃ³mo estÃ¡ implementada | Calidad |
|---------|--------------|------------------------|---------|
| **Claim Extraction** | âœ… SÃ­ | LLM via OpenRouter extrae claims del response | ğŸŸ¡ BÃ¡sica |
| **Claim Verification** | âœ… Parcial | LLM verifica claim vs context (NO es NLI real) | ğŸŸ¡ BÃ¡sica |
| **Source Relevance** | âœ… SÃ­ | Promedio de similarity scores del vector search | ğŸŸ¢ Correcta |
| **Source Coverage** | âœ… SÃ­ | Conteo de chunks + diversidad de CVs | ğŸŸ¢ Correcta |
| **Response Completeness** | âœ… SÃ­ | Checkea componentes del structured output | ğŸŸ¢ Correcta |
| **Internal Consistency** | âœ… Parcial | HeurÃ­sticas bÃ¡sicas (tablaâ†”conclusiÃ³n) | ğŸŸ¡ BÃ¡sica |
| **LLM-as-Judge** | âŒ NO | No implementado | âŒ |
| **NLI Models** | âŒ NO | Usamos LLM genÃ©rico, no modelo NLI especializado | âŒ |
| **Self-Consistency** | âŒ NO | Solo generamos 1 respuesta | âŒ |
| **Token Probabilities** | âŒ NO | OpenRouter no expone log_probs fÃ¡cilmente | âŒ |
| **Citation Verification** | âŒ NO | No generamos citas inline verificables | âŒ |
| **Answer Relevance** | âŒ NO | No medimos similitud queryâ†”response | âŒ |
| **Confidence Calibration** | âŒ NO | No tenemos histÃ³rico de feedback | âŒ |
| **RAGAS Metrics** | âŒ NO | No usamos el framework | âŒ |

### Veredicto Brutal

**Lo que implementÃ© antes es FUNCIONAL pero NO es nivel industria.**

- âœ… **SÃ­ es real**: Los scores vienen de datos reales (similarity scores, claim counts, etc.)
- âŒ **NO es avanzado**: Falta LLM-as-Judge, NLI, Self-Consistency, Answer Relevance
- ğŸŸ¡ **Es un 30%** de lo que hacen Sierra/Perplexity/Anthropic

---

## ğŸ“Š Gap Analysis Detallado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NUESTRA IMPLEMENTACIÃ“N ACTUAL                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Query â†’ [Guardrail] â†’ [Retrieval] â†’ [Generation] â†’ [Claim Verify] â†’       â”‚
â”‚          bÃ¡sico        similarity    LLM             LLM bÃ¡sico            â”‚
â”‚                        scores                                               â”‚
â”‚                                                                             â”‚
â”‚  Confidence = weighted_avg(                                                 â”‚
â”‚      source_coverage,      â† conteo de chunks (REAL pero simplista)        â”‚
â”‚      source_relevance,     â† avg similarity scores (REAL âœ“)                â”‚
â”‚      claim_verification,   â† LLM verifica claims (REAL pero NO es NLI)     â”‚
â”‚      response_completeness,â† checkea componentes (REAL âœ“)                  â”‚
â”‚      internal_consistency  â† heurÃ­sticas bÃ¡sicas (WEAK)                    â”‚
â”‚  )                                                                          â”‚
â”‚                                                                             â”‚
â”‚  âŒ FALTA:                                                                  â”‚
â”‚  â€¢ LLM-as-Judge evaluando Faithfulness/Relevance/Completeness              â”‚
â”‚  â€¢ NLI model para entailment real                                          â”‚
â”‚  â€¢ Answer Relevance (queryâ†”response similarity)                            â”‚
â”‚  â€¢ Self-Consistency (mÃºltiples samples)                                    â”‚
â”‚  â€¢ Token probability analysis                                              â”‚
â”‚  â€¢ Citation verification                                                   â”‚
â”‚  â€¢ Confidence calibration con feedback histÃ³rico                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LO QUE DEBERÃA SER (INDUSTRIA)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Query â†’ [Pre-Retrieval Evals] â†’ [Retrieval + Coverage Check] â†’            â”‚
â”‚          safety, intent          RAGAS context precision/recall            â”‚
â”‚                                                                             â”‚
â”‚        â†’ [Generation + Self-Assessment] â†’ [Post-Gen Evals] â†’               â”‚
â”‚          LLM genera + dice su confianza    LLM-as-Judge                    â”‚
â”‚                                            NLI Faithfulness                â”‚
â”‚                                            Answer Relevance                â”‚
â”‚                                            Citation Verify                 â”‚
â”‚                                                                             â”‚
â”‚        â†’ [Decision Engine] â†’ Response                                       â”‚
â”‚          â‰¥0.8: enviar                                                      â”‚
â”‚          â‰¥0.5: enviar con disclaimer                                       â”‚
â”‚          â‰¥0.3: regenerar                                                   â”‚
â”‚          <0.3: declinar                                                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Plan de ImplementaciÃ³n por Fases

### FASE 1: LLM-as-Judge (ALTO IMPACTO, MEDIA DIFICULTAD)
**Tiempo estimado: 2-3 dÃ­as**
**Cambio de arquitectura: NO**
**APIs adicionales: NO (usa OpenRouter existente)**

```python
# Nueva tÃ©cnica: Un LLM evalÃºa la respuesta de otro LLM

# Archivo: backend/app/services/llm_judge_service.py

JUDGE_PROMPT = """You are an expert evaluator for a CV screening RAG system.

CONTEXT (retrieved CV chunks):
{context}

QUESTION: {question}

RESPONSE TO EVALUATE: {response}

Evaluate on these criteria (1-5 scale):

1. FAITHFULNESS: Is every claim supported by the CV context?
2. RELEVANCE: Does the response answer the question asked?
3. COMPLETENESS: Are all parts of the question addressed?
4. List any HALLUCINATIONS (claims not in context)

Respond in JSON:
{
  "faithfulness": <int 1-5>,
  "relevance": <int 1-5>,
  "completeness": <int 1-5>,
  "hallucinations": [<list>],
  "confidence": <float 0-1>,
  "reasoning": "<explanation>"
}"""
```

**Impacto:**
- Reemplaza nuestro claim_verification simplista
- Un solo LLM call que evalÃºa TODO
- Mucho mÃ¡s robusto que verificar claim por claim

---

### FASE 2: Answer Relevance (ALTO IMPACTO, BAJA DIFICULTAD)
**Tiempo estimado: 1 dÃ­a**
**Cambio de arquitectura: NO**
**APIs adicionales: NO**

```python
# TÃ©cnica: Medir similitud semÃ¡ntica entre query y response

async def calculate_answer_relevance(
    query: str,
    response: str,
    embedder
) -> float:
    """
    Usa embeddings para medir si la respuesta es relevante a la pregunta.
    """
    query_embedding = await embedder.embed_text(query)
    response_embedding = await embedder.embed_text(response[:1000])  # Truncate
    
    # Cosine similarity
    similarity = cosine_similarity(query_embedding, response_embedding)
    
    return similarity  # 0.0 - 1.0
```

**Impacto:**
- Detecta respuestas que divagan o no contestan
- Reutiliza embedder existente
- Muy rÃ¡pido (solo 2 embeddings)

---

### FASE 3: Self-Consistency Light (MEDIO IMPACTO, MEDIA DIFICULTAD)
**Tiempo estimado: 2 dÃ­as**
**Cambio de arquitectura: MENOR (genera 2-3 responses)**
**APIs adicionales: NO**
**Costo: 2-3x mÃ¡s tokens**

```python
# TÃ©cnica: Generar N respuestas y medir consistencia

async def generate_with_consistency(
    prompt: str,
    llm,
    n_samples: int = 3,
    temperature: float = 0.7
) -> Tuple[str, float]:
    """
    Genera mÃºltiples respuestas y mide consistencia.
    """
    responses = []
    for _ in range(n_samples):
        resp = await llm.generate(prompt, temperature=temperature)
        responses.append(resp.text)
    
    # Extraer "key answer" de cada respuesta
    key_answers = [extract_key_answer(r) for r in responses]
    
    # Medir consistencia
    consistency = calculate_agreement(key_answers)
    
    # Usar respuesta con temperature=0 como final
    final_response = await llm.generate(prompt, temperature=0)
    
    return final_response.text, consistency
```

**Impacto:**
- Alta consistencia = alta confianza
- Detecta cuando el modelo estÃ¡ "adivinando"
- Trade-off: mÃ¡s latencia y costo

---

### FASE 4: NLI Faithfulness (ALTO IMPACTO, ALTA DIFICULTAD)
**Tiempo estimado: 3-5 dÃ­as**
**Cambio de arquitectura: SÃ (nuevo modelo)**
**APIs adicionales: SÃ - Hugging Face Inference API o modelo local**

```python
# TÃ©cnica: Modelo NLI especializado para verificar entailment

# OpciÃ³n A: Hugging Face Inference API
NLI_MODEL = "microsoft/deberta-v3-large-mnli"  # O cross-encoder/nli-deberta-v3-base

async def verify_claim_nli(
    claim: str,
    context: str,
    hf_api_key: str
) -> Tuple[str, float]:
    """
    Verifica si el contexto implica (entails) el claim.
    
    Returns:
        ("entailment" | "neutral" | "contradiction", confidence)
    """
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"https://api-inference.huggingface.co/models/{NLI_MODEL}",
            headers={"Authorization": f"Bearer {hf_api_key}"},
            json={
                "inputs": {
                    "premise": context,
                    "hypothesis": claim
                }
            }
        )
        result = response.json()
    
    # Result: [{"label": "ENTAILMENT", "score": 0.95}, ...]
    top_label = max(result, key=lambda x: x["score"])
    return top_label["label"].lower(), top_label["score"]

# OpciÃ³n B: Modelo local con transformers
from transformers import pipeline

nli_pipeline = pipeline("text-classification", model=NLI_MODEL)

def verify_claim_local(claim: str, context: str):
    result = nli_pipeline(f"{context} [SEP] {claim}")
    return result[0]["label"], result[0]["score"]
```

**Impacto:**
- VerificaciÃ³n de claims mucho mÃ¡s precisa que LLM genÃ©rico
- Modelos NLI estÃ¡n entrenados especÃ­ficamente para esto
- MÃ¡s rÃ¡pido y barato que llamadas LLM

**Requisitos:**
- API key de Hugging Face (gratis para uso moderado) O
- GPU local para modelo (4GB+ VRAM)

---

### FASE 5: Citation Verification (MEDIO IMPACTO, MEDIA DIFICULTAD)
**Tiempo estimado: 2-3 dÃ­as**
**Cambio de arquitectura: SÃ (cambiar prompt de generaciÃ³n)**
**APIs adicionales: NO**

```python
# Paso 1: Modificar prompt para que LLM genere con citas

GENERATION_PROMPT = """
Answer the question using ONLY the provided context.
IMPORTANT: Add inline citations [1], [2], etc. for every factual claim.

Context:
[1] {chunk_1}
[2] {chunk_2}
...

Question: {question}

Answer with citations:
"""

# Paso 2: Verificar cada cita

async def verify_citations(
    response: str,
    chunks: List[str]
) -> Tuple[float, List[dict]]:
    """
    Extrae citas del response y verifica cada una.
    """
    # Extraer citas: "claim [1]" â†’ claim, source_idx
    citation_pattern = r'([^.]+)\[(\d+)\]'
    citations = re.findall(citation_pattern, response)
    
    results = []
    for claim, source_idx in citations:
        source = chunks[int(source_idx) - 1]
        
        # Verificar con NLI o LLM
        is_supported = await verify_claim_nli(claim, source)
        
        results.append({
            "claim": claim,
            "source_idx": source_idx,
            "is_valid": is_supported[0] == "entailment",
            "confidence": is_supported[1]
        })
    
    valid_count = sum(1 for r in results if r["is_valid"])
    citation_score = valid_count / len(results) if results else 0
    
    return citation_score, results
```

---

### FASE 6: Decision Engine (MEDIO IMPACTO, BAJA DIFICULTAD)
**Tiempo estimado: 1 dÃ­a**
**Cambio de arquitectura: NO**
**APIs adicionales: NO**

```python
# LÃ³gica de decisiÃ³n basada en confidence

class DecisionEngine:
    THRESHOLDS = {
        "send": 0.80,
        "send_with_disclaimer": 0.50,
        "regenerate": 0.30,
        "decline": 0.0
    }
    
    def decide(
        self,
        confidence: float,
        faithfulness: float,
        has_contradictions: bool
    ) -> Tuple[str, Optional[str]]:
        """
        Decide quÃ© hacer con la respuesta.
        
        Returns:
            (action, disclaimer_text)
        """
        # Hard failures
        if has_contradictions:
            return "regenerate", None
        
        if faithfulness < 0.5:
            return "regenerate", None
        
        # Confidence-based decision
        if confidence >= self.THRESHOLDS["send"]:
            return "send", None
        
        elif confidence >= self.THRESHOLDS["send_with_disclaimer"]:
            return "send_with_disclaimer", (
                "âš ï¸ Esta respuesta tiene confianza moderada. "
                "Verifica la informaciÃ³n con los CVs originales."
            )
        
        elif confidence >= self.THRESHOLDS["regenerate"]:
            return "regenerate", None
        
        else:
            return "decline", (
                "No tengo suficiente informaciÃ³n en los CVs para "
                "responder esta pregunta con confianza."
            )
```

---

## ğŸ“‹ Resumen de Cambios Necesarios

### Cambios de Arquitectura

| Cambio | Severidad | DescripciÃ³n |
|--------|-----------|-------------|
| LLM-as-Judge | ğŸŸ¢ Menor | Nuevo service, no cambia flujo |
| Answer Relevance | ğŸŸ¢ Menor | Reutiliza embedder existente |
| Self-Consistency | ğŸŸ¡ Moderado | Genera mÃºltiples responses |
| NLI Model | ğŸŸ¡ Moderado | Nueva dependencia externa |
| Citation Verification | ğŸŸ¡ Moderado | Cambia prompt de generaciÃ³n |
| Decision Engine | ğŸŸ¢ Menor | Nueva lÃ³gica post-generaciÃ³n |

### Nuevas APIs/Keys Necesarias

| Servicio | Â¿Necesario? | Costo | Alternativa |
|----------|-------------|-------|-------------|
| Hugging Face API | Opcional | Gratis (rate limited) | Modelo local |
| OpenRouter | Ya tenemos | Variable | - |
| Modelo NLI local | Opcional | GPU 4GB+ | HF API |

### Impacto en Stack Actual

```
STACK ACTUAL:
â”œâ”€â”€ Backend: FastAPI + Python âœ… (no cambia)
â”œâ”€â”€ Frontend: React + Vite âœ… (no cambia)
â”œâ”€â”€ Vector DB: Supabase pgvector âœ… (no cambia)
â”œâ”€â”€ LLM: OpenRouter âœ… (no cambia)
â”œâ”€â”€ Embeddings: OpenRouter âœ… (no cambia)
â””â”€â”€ NUEVO: Hugging Face API (opcional) o modelo NLI local

CAMBIOS EN CÃ“DIGO:
â”œâ”€â”€ backend/app/services/
â”‚   â”œâ”€â”€ confidence_calculator.py   â†’ REESCRIBIR (integrar nuevos scores)
â”‚   â”œâ”€â”€ llm_judge_service.py       â†’ NUEVO
â”‚   â”œâ”€â”€ answer_relevance_service.py â†’ NUEVO
â”‚   â”œâ”€â”€ nli_verifier_service.py    â†’ NUEVO (si usamos NLI)
â”‚   â””â”€â”€ decision_engine.py         â†’ NUEVO
â”œâ”€â”€ backend/app/services/rag_service_v5.py â†’ MODIFICAR (integrar fases)
â””â”€â”€ frontend/src/components/MetricsPanel.jsx â†’ MODIFICAR (mostrar nuevos scores)
```

---

## ğŸ¯ RecomendaciÃ³n de ImplementaciÃ³n

### Orden por ROI (Return on Investment)

| Prioridad | TÃ©cnica | Esfuerzo | Impacto | ROI |
|-----------|---------|----------|---------|-----|
| 1ï¸âƒ£ | LLM-as-Judge | 2-3 dÃ­as | ğŸ”´ Alto | â­â­â­â­â­ |
| 2ï¸âƒ£ | Answer Relevance | 1 dÃ­a | ğŸ”´ Alto | â­â­â­â­â­ |
| 3ï¸âƒ£ | Decision Engine | 1 dÃ­a | ğŸŸ¡ Medio | â­â­â­â­ |
| 4ï¸âƒ£ | Citation Verification | 2-3 dÃ­as | ğŸŸ¡ Medio | â­â­â­ |
| 5ï¸âƒ£ | Self-Consistency | 2 dÃ­as | ğŸŸ¡ Medio | â­â­â­ |
| 6ï¸âƒ£ | NLI Faithfulness | 3-5 dÃ­as | ğŸ”´ Alto | â­â­â­ |

### MVP Recomendado (1 semana)

Implementar solo:
1. **LLM-as-Judge** - Reemplaza nuestro claim verification actual
2. **Answer Relevance** - Muy fÃ¡cil, alto impacto
3. **Decision Engine** - Comportamiento inteligente

Esto nos llevarÃ­a del **30% al ~60%** de lo que hace la industria.

### VersiÃ³n Completa (3-4 semanas)

AÃ±adir:
4. **Citation Verification** - Requiere cambiar prompts
5. **Self-Consistency** - Trade-off costo/precisiÃ³n
6. **NLI Model** - Requiere nueva integraciÃ³n

Esto nos llevarÃ­a al **~85%** de lo que hace la industria.

---

## â“ Preguntas para Decidir

1. **Â¿Priorizar velocidad o precisiÃ³n?**
   - Self-Consistency aÃ±ade 2-3x latencia
   - NLI es mÃ¡s preciso pero mÃ¡s lento que LLM-as-Judge

2. **Â¿Presupuesto para APIs adicionales?**
   - Hugging Face gratis tiene rate limits
   - Modelo local requiere GPU

3. **Â¿Aceptamos disclaimers en respuestas?**
   - Decision Engine puede mostrar advertencias
   - Â¿O preferimos solo respuestas de alta confianza?

4. **Â¿Queremos citas inline [1][2]?**
   - Cambia significativamente el formato de respuesta
   - MÃ¡s transparente pero mÃ¡s verbose

---

## ğŸ“ Archivos a Crear/Modificar

```
backend/app/services/
â”œâ”€â”€ evaluation/                          # NUEVO DIRECTORIO
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_judge.py                    # LLM-as-Judge service
â”‚   â”œâ”€â”€ answer_relevance.py             # Queryâ†”Response similarity
â”‚   â”œâ”€â”€ nli_verifier.py                 # NLI-based verification (opcional)
â”‚   â”œâ”€â”€ citation_verifier.py            # Citation checking
â”‚   â”œâ”€â”€ self_consistency.py             # Multiple samples
â”‚   â””â”€â”€ decision_engine.py              # Final decision logic
â”œâ”€â”€ confidence_calculator.py            # MODIFICAR - integrar nuevos scores
â””â”€â”€ rag_service_v5.py                   # MODIFICAR - llamar a nuevos services

frontend/src/components/
â””â”€â”€ MetricsPanel.jsx                    # MODIFICAR - mostrar breakdown detallado
```

---

## ConclusiÃ³n

**Â¿El cambio es drÃ¡stico?** 
- Arquitectura: NO, es incremental
- Stack: NO, mismo stack + 1 API opcional
- CÃ³digo: SÃ, varios servicios nuevos

**Â¿Necesitamos nuevas APIs?**
- MÃ­nimo: NO, todo puede hacerse con OpenRouter
- Ideal: SÃ, Hugging Face para NLI (gratis)

**Â¿Vale la pena?**
- LLM-as-Judge + Answer Relevance = **80% del beneficio con 20% del esfuerzo**
- NLI + Self-Consistency = **20% adicional con 80% del esfuerzo**

**RecomendaciÃ³n:** Implementar Fases 1-3 primero (1 semana), evaluar resultados, luego decidir si vale la pena las fases 4-6.
